
üìú Este projeto est√° licenciado sob a licen√ßa MIT - veja o arquivo LICENSE.md para detalhes. Nota: O dataset utilizado pertence √† Olist e foi obtido publicamente no Kaggle. Os direitos sobre os dados permanecem com a Olist [Acesso a fonte de dados original](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

üìå Cont√©m os c√≥digos em Python desenvolvidos para etapa de Data Cleaning. Os Scripts desenvolvidos no Mysql para consulta dos dados do banco. O arquivo do dashboard do Tableau utilizado para visulizar os dados trabalhados.


## üìå Vis√£o Geral do Projeto

Este projeto consiste em uma solu√ß√£o completa de **Full-Stack Analytics** aplicada ao contexto de E-commerce. O objetivo foi simular um cen√°rio real de engenharia e an√°lise de dados, partindo de dados brutos (Raw Data), passando por pipelines de ETL, modelagem de Data Warehouse e finalizando em um Dashboard Executivo para tomada de decis√£o.

O projeto utiliza o dataset p√∫blico da **Olist** (Brazilian E-Commerce Public Dataset), abrangendo pedidos, clientes, produtos e avalia√ß√µes de 2016 a 2018.

---

## üíº O Problema de Neg√≥cio

A Olist conecta pequenas empresas a grandes marketplaces. O desafio proposto foi responder a perguntas estrat√©gicas para a diretoria:
1.  **Performance Financeira:** Qual a evolu√ß√£o do faturamento (GMV) e Ticket M√©dio (AOV)?
2.  **Efici√™ncia Log√≠stica:** Onde o custo do frete impacta a margem e onde ocorrem os atrasos?
3.  **Experi√™ncia do Cliente (CX):** Existe correla√ß√£o entre atrasos na entrega e o NPS (Review Score)?
4.  **Pareto de Produtos:** Quais categorias impulsionam a receita?

---

## üõ†Ô∏è Tech Stack e Ferramentas

A arquitetura do projeto foi desenhada para garantir escalabilidade e integridade dos dados.

### üêç 1. Python (Engenharia & ETL)
Utilizado para orquestrar a ingest√£o de dados e limpeza avan√ßada.
* **IDE:** PyCharm (Vers√£o 2025.2.4).
* **Bibliotecas Utilizadas:**
    * `pandas`: Manipula√ß√£o de DataFrames, limpeza de strings e tratamento de nulos.
    * `sqlalchemy`: Cria√ß√£o da *engine* de conex√£o com o banco de dados (ORM).
    * `pymysql`: Driver para comunica√ß√£o Python-MySQL.
    * `unidecode`: Normaliza√ß√£o de textos (remo√ß√£o de acentos e caracteres especiais em nomes de cidades e categorias).
    * `os`: Gerenciamento de diret√≥rios e caminhos de arquivos para automa√ß√£o.

### üóÑÔ∏è 2. Banco de Dados (Data Warehousing)
* **Servidor Local:** XAMPP (Apache + MariaDB/MySQL). Escolhido pela facilidade de criar um ambiente de servidor local robusto.
* **Interface de Gerenciamento:** MySQL Workbench. Utilizado para:
    * Modelagem do Schema (DDL).
    * Cria√ß√£o de √çndices e Chaves (PKs e FKs).
    * Valida√ß√£o de integridade referencial.
    * Consultas ad-hoc (SQL) para valida√ß√£o de m√©tricas.

### üìä 3. Visualiza√ß√£o (Data Viz)
* **Ferramenta:** Tableau Public.
* **Modelagem:** Star Schema (Esquema Estrela) com tabela fato "order_items" centralizada. A conex√£o com as demais tabelas foram feitas por meio da defini√ß√£o das PK estabelecidas no Mysql.
* **Features:** Par√¢metros din√¢micos (Top N categorias vendidas), Campos Calculados e A√ß√µes de Filtro interativas.

### üêô 4. Versionamento
* **Ferramenta:** GitHub Desktop.
* **Controle:** Versionamento de scripts Python e queries SQL para garantir rastreabilidade do c√≥digo.

---

## ‚öôÔ∏è Metodologia e Etapas de Execu√ß√£o

### FASE 1: Engenharia de Dados (ETL)
* **Teste de conex√£o com o MYSQL:** [Cria√ß√£o do arquivo 'teste_conexao_1.py'](https://github.com/eaiyukenish40-cloud/Projeto-Portifolio-Olist/blob/main/DataCleaning_Python/teste_conexao_1.py)
* **Ingest√£o:** Cria√ß√£o de script Python para leitura automatizada de m√∫ltiplos arquivos CSV (Source: Kaggle Olist) sendo importados posteriormente dentro do Mysql [script 'carrega dados2.py':](https://github.com/eaiyukenish40-cloud/Projeto-Portifolio-Olist/blob/main/DataCleaning_Python/carrega%20dados_2.py])
* * **Investiga√ß√£o dos dados:** Com os arquivos importados no Mysql, foram contru√≠das queries para cada tabela para visualiza√ß√£o dos dados dispon√≠veis [Pasta SQL](https://github.com/eaiyukenish40-cloud/Projeto-Portifolio-Olist/tree/main/Mysql/Scripts_SQL)
* **Data Cleaning:**
    * Tratamento de tipos de dados (convers√£o de datas, floats e strings).
    * `sqlalchemy` para conex√£o com os dados e `pandas` para obter os dataframes, leitura, importa√ß√£o em csv, escrita no mysql.
    * Normaliza√ß√£o de texto com `unidecode` para padronizar cidades (ex: "s√£o paulo" -> "sao paulo").
    * Implementa√ß√£o de **Carga Fracionada (Chunking)** e m√©todo `multi` para otimizar a inser√ß√£o de milhares de linhas no MySQL sem travar a mem√≥ria.
    * Uso da importa√ß√£o de importa√ß√£o de dados do Mysql para uso dos CSV's limpos no Tableau P√∫blic ['importa_mysql_4'](https://github.com/eaiyukenish40-cloud/Projeto-Portifolio-Olist/blob/main/DataCleaning_Python/importa_mysql_4.py)

### FASE 2: Modelagem de Dados (SQL)
* Transforma√ß√£o de tabelas "soltas" em um **Modelo Relacional**.
* Defini√ß√£o de **Primary Keys** simples e compostas (ex: `order_items` possui PK composta por `order_id` + `order_item_id`).
* Cria√ß√£o de **Foreign Keys** para garantir a integridade do banco (impedir pedidos sem clientes, etc.).
* Cria√ß√£o de tabela resumida de geolocaliza√ß√£o (`geo_resumida`) para otimizar a performance do mapa no Tableau.

### FASE 3: Analytics & Storytelling (Tableau)
* Desenvolvimento de Dashboard Executivo com layout fixo (UX).
* Cria√ß√£o de an√°lises YoY (Year over Year) para comparar 2017 vs 2018 (per√≠odos entre janeiro a agosto).
* An√°lise de correla√ß√£o entre **Prazo de Entrega** e **Nota de Avalia√ß√£o**.

---

## üìà Resultados e Dashboard

O resultado final √© um painel interativo que permite filtrar por Estado, Ano e Categoria.

üîó **[Clique aqui para acessar o Dashboard Interativo no Tableau Public](https://public.tableau.com/app/profile/gustavo.maizatto/viz/DataViz_ProjetoPortifolio/VisoGeralVendas?publish=yes)**

*<img width="1356" height="753" alt="image" src="https://github.com/user-attachments/assets/16978c6f-b33f-40f4-98cb-4af11654adc4" />*

**Principais Insights:**
* **Sazonalidade:** Pico agressivo de vendas identificado na Black Friday de 2017.
* **Log√≠stica:** O Sudeste possui o menor custo de frete relativo, enquanto Norte e Nordeste sofrem com custos altos, impactando diretamente na satisfa√ß√£o.
* **Qualidade:** Pedidos entregues com atraso t√™m uma nota m√©dia ~50% menor que pedidos no prazo.
* **Aumento nas vendas:** √â not√°vel a evolu√ß√£o no aumento das vendas em 2018, entre os per√≠odos com dados dispon√≠veis, e pode-se notar uma piora na qualidade da margem frete em 2018 em compara√ß√£o com 2017. Uma poss√≠vel sobrecarga na demanda x qualidade?

## üßó Desafios e Li√ß√µes Aprendidas

Durante o desenvolvimento, enfrentei desafios reais de engenharia que exigiram adapta√ß√µes na arquitetura:

* **Infraestrutura e Resili√™ncia:** O ambiente de servidor local (XAMPP/MySQL) apresentou instabilidades durante cargas massivas de dados.
    * *Li√ß√£o:* A import√¢ncia cr√≠tica de rotinas de **Backup** e a separa√ß√£o dos arquivos de dados (`ibdata`) para recupera√ß√£o de desastres.
* **Limita√ß√£o de Conectividade:** O Tableau Public encerrou o suporte a conex√µes *live* com bancos locais.
    * *Solu√ß√£o (Workaround):* Implementei um pipeline intermedi√°rio em Python para exportar as tabelas processadas em arquivos `.csv` otimizados, simulando um Data Lake est√°tico.  ['importa_mysql_4'](https://github.com/eaiyukenish40-cloud/Projeto-Portifolio-Olist/blob/main/DataCleaning_Python/importa_mysql_4.py)
* **Qualidade de Dados (Data Quality):**
    * Havia inconsist√™ncias graves na grafia de cidades (input manual do usu√°rio).
    * *Estrat√©gia:* Apliquei a **Regra de Pareto (80/20)** para limpar os maiores ofensores via script, garantindo a integridade das regi√µes principais.
    * *Melhoria Futura:* Import√¢ncia de implementar constraints de valida√ß√£o na ponta da coleta (Front-end) para evitar "lixo na entrada" (Garbage In, Garbage Out).

---

## üöÄ Como reproduzir este projeto

1.  Clone este reposit√≥rio.
2.  Instale as depend√™ncias: `pip install pandas sqlalchemy pymysql unidecode`.
3.  Configure seu servidor MySQL local (XAMPP) e crie um banco chamado `olist`.
4.  Ajuste a string de conex√£o no arquivo `config.py` (ou script principal).
5.  Execute os scripts de carga na ordem num√©rica.
